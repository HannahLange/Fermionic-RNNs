{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b483b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e68cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb9b0a809d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial settings\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")\n",
    "random.seed(1234)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4bada",
   "metadata": {},
   "source": [
    "## 1. Build the 2D RNN\n",
    "- tensorized RNN cell as in Hibat-Allah 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3588d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, system_size_x, system_size_y, hidden_dim, n_layers, sz_tot = None):\n",
    "        super(Model, self).__init__()\n",
    "        \"\"\"\n",
    "        Creates RNN consisting of GRU cells.\n",
    "        Inputs:\n",
    "            - input_size:  number of quantum numbers (i.e. 2 for spin-1/2 particles)\n",
    "            - system_size: length of each snapshot\n",
    "            - hidden_dim:  dimension of hidden states\n",
    "            - n_layers:    number of layers of the GRU\n",
    "        \"\"\"\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size    # number of expected features in input data\n",
    "        self.output_size = input_size    # number of expected features in output data\n",
    "        self.N_x         = system_size_x # length of generated samples in x dir\n",
    "        self.N_y         = system_size_y # length of generated samples in x dir\n",
    "        self.hidden_dim  = hidden_dim    # number of features in the hidden state\n",
    "        self.n_layers    = n_layers      # number of stacked GRUs\n",
    "        self.sz_tot      = sz_tot        # total magnetization if u(1) symmetry is applied (default: None)\n",
    "        self.system_size = system_size_x*system_size_y\n",
    "        #Defining the layers\n",
    "        self.rnn  = nn.GRU(self.input_size, hidden_dim*2, n_layers, batch_first=True)   \n",
    "        self.lin1 = nn.Linear(hidden_dim, self.output_size)\n",
    "        self.lin2 = nn.Linear(hidden_dim, self.output_size)\n",
    "        #self.s    = torch.softmax(dim=0)\n",
    "        self.soft = nn.Softsign()\n",
    "        \n",
    "        self.get_num_parameters()\n",
    "        \n",
    "    def forward(self, x, hidden = None):\n",
    "        \"\"\"\n",
    "        Passes the input through the network.\n",
    "        Inputs:\n",
    "            - x:      input state at t\n",
    "            - hidden: hidden state at t\n",
    "        Outputs:\n",
    "            - out:    output configuration at t+1\n",
    "            - hidden: hidden state at t+1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the dense layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generates the hidden state for a given batch size.\n",
    "        \"\"\"\n",
    "        # This method generates the first hidden state of zeros for the forward pass and passes it to the device.\n",
    "        # This is equivalent to a product state.\n",
    "        hidden = torch.zeros((self.n_layers, batch_size, self.hidden_dim), dtype=torch.float64).to(device)\n",
    "        return hidden\n",
    "    \n",
    "    def get_num_parameters(self):\n",
    "        \"\"\"\n",
    "        Calculates the number of parameters of the network. \"\"\"\n",
    "        p = 0\n",
    "        for param in list(self.parameters()):\n",
    "            if param.requires_grad:\n",
    "                p += param.numel()\n",
    "        print(\"Total number of parameters in the network: \"+str(p))\n",
    "        return p\n",
    "    \n",
    "    def enforce_sz_total(self, samples, amplitudes):\n",
    "        bl = (self.system_size//2)*torch.ones((samples.size()[0],1))\n",
    "        s_dn = samples.clone().detach()\n",
    "        s_dn[samples == 0] = -1\n",
    "        s_dn[samples == 1] = 0\n",
    "        num_up  = torch.sum(samples, axis=1)\n",
    "        num_dn  = - torch.sum(s_dn, axis=1)\n",
    "\n",
    "        ampl_up = torch.heaviside(bl-num_up, torch.tensor([0.]))\n",
    "        ampl_dn = torch.heaviside(bl-num_dn, torch.tensor([0.]))\n",
    "        \n",
    "        ampl = amplitudes * torch.stack([ampl_dn, ampl_up], axis=1)[:,:,0]\n",
    "        ampl = torch.nn.functional.normalize(ampl, p=2, eps = 1e-30)\n",
    "        return ampl\n",
    "    \n",
    "    def _gen_samples(self, nx, ny, direction, samples, ampl_probs, phase_probs, ohs, inputs, hidden_inputs, numsamples):\n",
    "        # pass the hidden unit and sigma into the GRU cell at t=i \n",
    "        # and get the output y (will be used for calculating the \n",
    "        # probability) and the next hidden state\n",
    "        print(inputs[str(nx+direction[0])+str(ny)].size())\n",
    "        print(inputs[str(nx)+str(ny+direction[1])].size())\n",
    "        full_sigma = torch.stack([inputs[str(nx+direction[0])+str(ny)],inputs[str(nx)+str(ny+direction[1])]], axis=1)[:,:,0,:]\n",
    "        print(full_sigma.size())\n",
    "        hidden     = torch.stack([hidden_inputs[str(nx+direction[0])+str(ny)],hidden_inputs[str(nx)+str(ny+direction[1])]], axis=-1)\n",
    "        hidden     = torch.reshape(hidden, (self.n_layers, numsamples, self.hidden_dim*2))\n",
    "        y, hidden  = self.forward(full_sigma, hidden)\n",
    "        print(y.size())\n",
    "        print(hidden.size())\n",
    "        # the amplitude is given by a linear layer with a softmax activation\n",
    "        ampl = self.lin1(y)\n",
    "        ampl = torch.softmax(ampl,dim=1) # amplitude, all elements in a row sum to 1\n",
    "        # the phase is given by a linear layer with a softsign activation\n",
    "        phase = self.lin2(y)\n",
    "        phase = self.soft(phase) \n",
    "        phase_probs[nx][ny] = torch.mul(torch.pi,phase)\n",
    "        # samples are obtained by sampling from the amplitudes\n",
    "        \"\"\"\n",
    "        if self.sz_tot != None and i>=self.system_size/2:\n",
    "            ampl = self.enforce_sz_total(torch.stack(samples, axis=1), ampl)\n",
    "        \"\"\"\n",
    "        ampl_probs[nx][ny] = ampl\n",
    "        sample = torch.multinomial(ampl, 1)\n",
    "        samples[nx][ny] = sample\n",
    "        # one hot encode the current sigma to pass it into the GRU at\n",
    "        # the next time step\n",
    "        sigma = nn.functional.one_hot(sample, 2).double()\n",
    "        ohs[nx][ny] = sigma\n",
    "        inputs[str(nx)+str(ny)] = sigma\n",
    "        \n",
    "        return samples, inputs, ampl_probs, phase_probs, ohs\n",
    "    \n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generates num_samples samples from the network and returns the samples,\n",
    "        their log probabilities and phases.\n",
    "        \"\"\"\n",
    "        # generate a first input of zeros (sigma and hidden states) to the first GRU cell at t=0\n",
    "        sigma       = torch.zeros((num_samples,1,2), dtype=torch.float64).to(device)\n",
    "        inputs = {}\n",
    "        hidden_inputs = {}\n",
    "        for ny in range(-2, self.N_y):\n",
    "            for nx in range(-2, self.N_x):\n",
    "                inputs[str(nx)+str(ny)] = sigma\n",
    "                hidden_inputs[str(nx)+str(ny)] = self.init_hidden(num_samples)\n",
    "                \n",
    "        samples     = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        ampl_probs  = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        phase_probs = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        ohs         = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        for ny in range(self.N_y):\n",
    "            if ny % 2 == 0: #go from left to right\n",
    "                for nx in range(self.N_x):\n",
    "                    direction = [-1,-1]\n",
    "                    samples, inputs, ampl_probs, phase_probs, ohs = self._gen_samples(nx, ny, direction, samples, ampl_probs, phase_probs, ohs, inputs, hidden_inputs, num_samples)\n",
    "            else: #go from right to left\n",
    "                for nx in range(self.N_x-1, -1, -1):\n",
    "                    direction = [1,-1]\n",
    "                    samples, inputs, ampl_probs, phase_probs, ohs = self._gen_samples(nx, ny, direction, samples, ampl_probs, phase_probs, ohs, inputs, hidden_inputs, num_samples)\n",
    "        \n",
    "        samples = torch.stack(samples, axis=1)\n",
    "        ampl_probs = torch.transpose(torch.stack(ampl_probs, axis = 2), 1, 2)\n",
    "        phase_probs = torch.transpose(torch.stack(phase_probs, axis = 2), 1, 2)\n",
    "        ohs = torch.stack(ohs, axis = 2)[:,0,:,:]\n",
    "        # calculate the wavefunction and split it into amplitude and phase\n",
    "        log_probs_ampl = torch.sum(torch.log(torch.sum(torch.torch.multiply(ampl_probs,ohs), axis =2)), axis=1)\n",
    "        phase = torch.sum((torch.sum(torch.torch.multiply(phase_probs,ohs), axis =2)), axis=1)\n",
    "        return samples, log_probs_ampl, phase\n",
    "    \n",
    "    def _gen_probs(input1, input2, samples, ampl_probs, phase_probs, inputs, hidden_inputs, numsamples):\n",
    "        # pass the hidden unit and sigma into the GRU cell at t=i \n",
    "        # and get the output y (will be used for calculating the \n",
    "        # probability) and the next hidden state\n",
    "        \n",
    "        full_sigma = [inputs[str(input1)+str(ny)], inputs[str(nx)+str(input2)]]\n",
    "        print(full_sigma)\n",
    "        hidden     = [hidden_inputs[str(input1)+str(ny)],hidden_inputs[str(nx)+str(input2)]]\n",
    "        y, hidden  = self.forward(full_sigma, hidden)\n",
    "        # the amplitude is given by a linear layer with a softmax activation\n",
    "        ampl = self.lin1(y)\n",
    "        ampl = torch.softmax(ampl,dim=1) # amplitude, all elements in a row sum to 1\n",
    "        \"\"\"\n",
    "        if self.sz_tot != None and i>=self.system_size/2:\n",
    "            ampl = self.enforce_sz_total(torch.stack(samples, axis=1), ampl)\n",
    "        \"\"\"\n",
    "        ampl_probs[nx][ny] = ampl\n",
    "        # the phase is given by a linear layer with a softsign activation\n",
    "        phase = self.lin2(y)\n",
    "        phase = self.soft(phase) \n",
    "        phase_probs[nx][ny] = torch.mul(torch.pi,phase)\n",
    "        # one hot encode the current sigma to pass it into the GRU at\n",
    "        # the next time step\n",
    "        inputs[str(nx)+str(ny)] = nn.functional.one_hot(samples[:,i,:], 2).double()\n",
    "        \n",
    "        return samples, ampl_probs, phase_probs\n",
    "    \n",
    "    def log_probabilities(self, samples):\n",
    "        \"\"\"\n",
    "        Calculates the log probability and the phase of each item in samples.\n",
    "        \"\"\"\n",
    "        inputs = {}\n",
    "        hidden_inputs = {}\n",
    "        for ny in range(self.N_y):\n",
    "            for nx in range(self.N_x):\n",
    "                inputs[str(nx)+str(ny)] = sigma\n",
    "                hidden_inputs[str(nx)+str(ny)] = self.init_hidden(num_samples)\n",
    "                \n",
    "        ampl_probs  = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        phase_probs = [[[] for nx in range(self.N_x)] for ny in range(self.N_y)]\n",
    "        for ny in range(self.N_y):\n",
    "            if ny % 2 == 0: #go from left to right\n",
    "                for nx in range(self.N_x):\n",
    "                    samples, ampl_probs, phase_probs = self._gen_probs(nx-1, ny-1, samples, ampl_probs, phase_probs, inputs, hidden_inputs)\n",
    "            else: #go from right to left\n",
    "                for nx in range(self.N_x-1, -1, -1):\n",
    "                    samples, ampl_probs, phase_probs = self._gen_probs(nx+1, ny-1, samples, ampl_probs, phase_probs, inputs, hidden_inputs)   \n",
    "        \n",
    "        ampl_probs = torch.transpose(torch.stack(ampl_probs, axis = 2), 1, 2)\n",
    "        phase_probs = torch.transpose(torch.stack(phase_probs, axis = 2), 1, 2)   \n",
    "        ohs = nn.functional.one_hot(samples, 2)[:,:,0,:]\n",
    "        log_probs_ampl = torch.sum(torch.log(torch.sum(torch.torch.multiply(ampl_probs,ohs), axis =2)), axis=1)\n",
    "        phase = torch.sum((torch.sum(torch.torch.multiply(phase_probs,ohs), axis =2)), axis=1)\n",
    "        return log_probs_ampl, phase\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9067d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the network: 3124\n",
      "Model(\n",
      "  (rnn): GRU(2, 30, batch_first=True)\n",
      "  (lin1): Linear(in_features=15, out_features=2, bias=True)\n",
      "  (lin2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  (soft): Softsign()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "systemsize = 4\n",
    "hiddendim  = 15\n",
    "numsamples = 10\n",
    "\n",
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=2, system_size_x=4, system_size_y = 4, hidden_dim=hiddendim, n_layers=1, sz_tot=0)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "44615675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 2])\n",
      "torch.Size([10, 1, 2])\n",
      "torch.Size([10, 2, 2])\n",
      "torch.Size([40, 15])\n",
      "torch.Size([1, 10, 30])\n",
      "torch.Size([40, 1, 2])\n",
      "torch.Size([10, 1, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [40, 1, 2] at entry 0 and [10, 1, 2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_1234/2661714434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#test the sampling method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumsamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_1234/2771345510.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mampl_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mohs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mampl_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mohs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#go from right to left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mnx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_x\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_1234/2771345510.py\u001b[0m in \u001b[0;36m_gen_samples\u001b[0;34m(self, nx, ny, direction, samples, ampl_probs, phase_probs, ohs, inputs, hidden_inputs, numsamples)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mfull_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_sigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mhidden\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [40, 1, 2] at entry 0 and [10, 1, 2] at entry 1"
     ]
    }
   ],
   "source": [
    "#test the sampling method\n",
    "samples, log_probs, phase = model.sample(numsamples)\n",
    "print(log_probs.size())\n",
    "samples = torch.unique(samples, dim=0)\n",
    "print(samples)\n",
    "\n",
    "2**systemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4221f14e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_1234/994341709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test the probability method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "# test the probability method\n",
    "log_probs, phases = model.log_probabilities(samples)\n",
    "print(phases)\n",
    "print(log_probs)\n",
    "print(torch.sum(torch.mul(torch.exp(0.5*log_probs+1j*phases),torch.exp(0.5*log_probs+1j*phases).conj())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb03b4c",
   "metadata": {},
   "source": [
    "### 2. Calculate the matrix elements (here 2D XXZ model)\n",
    "\n",
    "$$ E_{\\theta}^{loc}(x) = \\frac{<x|H|\\psi_\\theta>}{<x|\\psi_\\theta>} = H_{diag}(x)+H_{offd}(x)\\frac{<x^{\\prime}|\\psi_\\theta>}{<x|\\psi_\\theta>} $$\n",
    "with $\\hat{H}_{offd}|x^{\\prime}>=H_{offd}(x)|x^{\\prime}>$ and ${<x|\\psi_\\theta>}$ given by the square root of the exponential of model.log_probabilities(x) defined above.\n",
    "\n",
    "- for $J_p = 0$ and $J_z = 1$: $E_{loc} = H_{diag}(x) = 0.25*J_z*systemsize$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f77b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XXZ1D_MatrixElements(Jp, Jz, samples, length):\n",
    "    \"\"\" \n",
    "    Calculate the local energies of 1D XXZ model given a set of set of samples.\n",
    "    Returns: The local energies that correspond to the input samples.\n",
    "    Inputs:\n",
    "    - sample: (num_samples, N)\n",
    "    - Jp: float\n",
    "    - Jz: float\n",
    "    \"\"\"\n",
    "\n",
    "    N = samples.size()[1]\n",
    "    numsamples = samples.size()[0]\n",
    "    \n",
    "    #diagonal elements\n",
    "    diag_matrixelements = torch.zeros((numsamples, length))\n",
    "    #diagonal elements from the SzSz term \n",
    "    for i in range(length): \n",
    "        values  = samples[:,i]+samples[:,(i+1)%N]\n",
    "        valuesT = values.clone()\n",
    "        valuesT[values==2] = +1 #If both spins are up\n",
    "        valuesT[values==0] = +1 #If both spins are down\n",
    "        valuesT[values==1] = -1 #If they are opposite\n",
    "        diag_matrixelements[:,i] = valuesT.reshape((numsamples))*Jz*0.25\n",
    "    \n",
    "    #off-diagonal elements from the S+S- terms\n",
    "    offd_matrixelements = torch.zeros((numsamples, length))\n",
    "    xprime = []\n",
    "    for i in range(length): \n",
    "        values = samples[:,i]+samples[:,(i+1)%N]\n",
    "        valuesT = values.clone()\n",
    "        #flip the spins\n",
    "        new_samples             = samples.clone()\n",
    "        new_samples[:,(i+1)%N]  = samples[:,i]\n",
    "        new_samples[:,i]        = samples[:,(i+1)%N]\n",
    "        valuesT[values==2]      = 0 #If both spins are up\n",
    "        valuesT[values==0]      = 0 #If both spins are down\n",
    "        valuesT[values==1]      = 1 #If they are opposite\n",
    "        offd_matrixelements[:,i] = valuesT.reshape((numsamples))*Jp*0.5\n",
    "        xprime.append(new_samples)\n",
    "    return diag_matrixelements, offd_matrixelements, torch.stack(xprime, axis=0)\n",
    "\n",
    "def XXZ1D_Eloc(Jp, Jz, samples, RNN, boundaries):\n",
    "    \"\"\" \n",
    "    Calculate the local energies of 1D XXZ model given a set of set of samples.\n",
    "    Returns: The local energies that correspond to the input samples.\n",
    "    Inputs:\n",
    "    - sample: (num_samples, N)\n",
    "    - Jp: float\n",
    "    - Jz: float\n",
    "    - boundaries: str, open or periodic\n",
    "    \"\"\"\n",
    "\n",
    "    N          = samples.size()[1]\n",
    "    numsamples = samples.size()[0]\n",
    "    if boundaries == \"periodic\":\n",
    "        length = N\n",
    "    else:\n",
    "        length = N-1\n",
    "    \n",
    "    queue_samples       = torch.zeros((length+1, numsamples, N, 1), dtype = torch.int32) \n",
    "    log_probs           = np.zeros((length+1)*numsamples, dtype=np.float64) \n",
    "    \n",
    "    #matrix elements\n",
    "    diag_me, offd_me, new_samples = XXZ1D_MatrixElements(Jp, Jz, samples, length)\n",
    "    diag_me = torch.sum(diag_me, axis=1)\n",
    "    offd_me = offd_me.to(torch.complex64)\n",
    "    # diagonal elements\n",
    "    queue_samples[0] = samples\n",
    "    Eloc = diag_me.to(torch.complex64)\n",
    "    #off-diagonal elements\n",
    "    \n",
    "    offd_Eloc = np.zeros((numsamples), dtype = np.float64)\n",
    "    queue_samples[1:] = new_samples\n",
    "    queue_samples_reshaped = np.reshape(queue_samples, [(length+1)*numsamples, N, 1])\n",
    "    log_probs, phases = model.log_probabilities(queue_samples_reshaped.to(torch.int64))\n",
    "    log_probs_reshaped = torch.reshape(log_probs, (length+1,numsamples)).to(torch.complex64)\n",
    "    phases_reshaped = torch.reshape(phases, (length+1,numsamples))\n",
    "    for i in range(1,length+1):\n",
    "        tot_log_probs = 0.5*(log_probs_reshaped[i,:]-log_probs_reshaped[0,:])\n",
    "        tot_log_probs += 1j*(phases_reshaped[i,:]-phases_reshaped[0,:])\n",
    "        Eloc += offd_me[:,i-1]*(torch.exp(tot_log_probs))\n",
    "    return Eloc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "32cdfd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.]]], dtype=torch.float64)\n",
      "tensor([nan+nanj, nan+nanj], grad_fn=<AddBackward0>)\n",
      "AFM\n",
      "tensor([[[1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]], dtype=torch.float64)\n",
      "tensor([nan+nanj, nan+nanj], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#simple tests\n",
    "Jp = 1\n",
    "Jz = 1\n",
    "boundaries = \"open\"\n",
    "\n",
    "\"\"\"\n",
    "samples, log_probs, phase = model.sample(10)\n",
    "local_energy = XXZ1D_Eloc(Jp, Jz, samples, model, boundaries)\n",
    "print(local_energy)\n",
    "\n",
    "\"\"\"\n",
    "# FM sample\n",
    "print(\"FM\")\n",
    "samples = torch.zeros((2, 10,1), dtype = torch.float64)\n",
    "samples[1,-2,0] = 1\n",
    "print(samples)\n",
    "local_energy = XXZ1D_Eloc(Jp, Jz, samples, model, boundaries)\n",
    "print(local_energy)\n",
    "\n",
    "# AFM sample\n",
    "print(\"AFM\")\n",
    "samples = torch.zeros((2, 10,1), dtype = torch.float64)\n",
    "for i in range(0,samples.size()[1],2):\n",
    "    samples[:,i,:] = 1\n",
    "samples[1,-3,0] = 1\n",
    "print(samples)\n",
    "local_energy = XXZ1D_Eloc(Jp, Jz, samples, model, boundaries)\n",
    "print(local_energy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f79c8c",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e5aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(1234)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c872de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the network: 919\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "Jp         = 1\n",
    "Jz         = 1\n",
    "systemsize = 8\n",
    "bounds     = \"open\"\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs   = 3000\n",
    "lr         = 0.01\n",
    "hidden_dim = systemsize\n",
    "\n",
    "model = Model(input_size=2, system_size=systemsize, hidden_dim=hiddendim, n_layers=1, sz_tot=0)\n",
    "model = model.to(device)\n",
    "model = model.double()\n",
    "print(model.sz_tot)\n",
    "\n",
    "\n",
    "# Optimizer and cost function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def cost_fct(samples, model, Jp, Jz, log_probs, phases, boundaries):\n",
    "    Eloc = XXZ1D_Eloc(Jp, Jz, samples, model, boundaries)\n",
    "    log_psi = (0.5*log_probs+1j*phases)\n",
    "    eloc_sum = (Eloc).mean(axis=0) #/samples.size()[0]\n",
    "    e_loc_corr = (Eloc - eloc_sum).detach()\n",
    "    cost = 2 * torch.real((torch.conj(log_psi) * e_loc_corr.to(torch.complex128))).mean(axis=0)\n",
    "    return eloc_sum, cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# observables that can be evaluated during the training or afterwards\n",
    "def get_szsz(samples, log_probs, boundaries):\n",
    "    N = samples.size()[1]\n",
    "    if boundaries == \"periodic\":\n",
    "        length = N\n",
    "    else:\n",
    "        length = N-1\n",
    "    szsz = torch.zeros((samples.size()[0], length))\n",
    "    s = samples.clone().detach() \n",
    "    s[samples == 0] = -1\n",
    "    for i in range(length):\n",
    "        szsz[:,i] = s[:,i,0]*s[:,(i+1)%N,0]\n",
    "    print(szsz.size())\n",
    "    return torch.mean(szsz, axis=0)*1/4\n",
    "\n",
    "def get_sxsx(samples, log_probs, phases, boundaries):\n",
    "    N = samples.size()[1]\n",
    "    if boundaries == \"periodic\":\n",
    "        length = N\n",
    "    else:\n",
    "        length = N-1\n",
    "    sxsx = torch.zeros((samples.size()[0], length))\n",
    "    for i in range(length):\n",
    "        s1 = flip_neighbor_spins(samples, i)\n",
    "        log_probs1, phases1 = model.log_probabilities(s1)\n",
    "        sxsx[:,i] = torch.exp(0.5*(log_probs1-log_probs))*torch.exp(1j*(phases1-phases))\n",
    "    return torch.mean(sxsx, axis=0)*1/4\n",
    "\n",
    "def get_sysy(samples, log_probs, phases, boundaries):\n",
    "    N = samples.size()[1]\n",
    "    if boundaries == \"periodic\":\n",
    "        length = N\n",
    "    else:\n",
    "        length = N-1\n",
    "    sysy = torch.zeros((samples.size()[0], length))\n",
    "    for i in range(length):\n",
    "        s1 = flip_neighbor_spins(samples, i)\n",
    "        log_probs1, phases1 = model.log_probabilities(s1)\n",
    "        s1 = s1.to(torch.complex64)\n",
    "        s1[:,i,0][s1[:,i,0] == 1] = -1j\n",
    "        s1[:,i,0][s1[:,i,0] == 0] = 1j\n",
    "        s1[:,(i+1)%N,0][s1[:,(i+1)%N,0] == 1] = -1j\n",
    "        s1[:,(i+1)%N,0][s1[:,(i+1)%N,0] == 0] = 1j\n",
    "        sysy[:,i] = torch.exp(0.5*(log_probs1-log_probs))*torch.exp(1j*(phases1-phases))*s1[:,i,0]*s1[:,(i+1)%N,0]\n",
    "    return torch.mean(sysy, axis=0)*1/4\n",
    "\n",
    "def get_sz(samples):\n",
    "    N = samples.size()[1]\n",
    "    sz = torch.zeros((samples.size()[0], N))\n",
    "    s = samples.clone().detach() \n",
    "    s[samples == 0] = -1\n",
    "    sz = s[:,:,0].to(torch.float64)\n",
    "    return torch.sum(torch.mean(sz, axis=0)*1/2)\n",
    "\n",
    "def get_sx(samples, log_probs, phases):\n",
    "    N = samples.size()[1]\n",
    "    sx = torch.zeros((samples.size()[0], N))\n",
    "    for i in range(N):\n",
    "        s1 = flip_spin(samples, i)\n",
    "        log_probs1, phases1 = model.log_probabilities(s1)\n",
    "        sx[:,i] = torch.exp(0.5*(log_probs1-log_probs))*torch.exp(1j*(phases1-phases))\n",
    "    return torch.sum(torch.mean(sx, axis=0)*1/2)\n",
    "\n",
    "def get_sy(samples, log_probs, phases):\n",
    "    N = samples.size()[1]\n",
    "    sy = torch.zeros((samples.size()[0], N))\n",
    "    for i in range(N):\n",
    "        s1 = flip_spin(samples, i)\n",
    "        log_probs1, phases1 = model.log_probabilities(s1)\n",
    "        s1 = s1.to(torch.complex64)\n",
    "        s1[:,i,0][s1[:,i,0] == 1] = -1j\n",
    "        s1[:,i,0][s1[:,i,0] == 0] = 1j\n",
    "        sy[:,i] = torch.exp(0.5*(log_probs1-log_probs))*torch.exp(1j*(phases1-phases))*s1[:,i,0]\n",
    "    return torch.sum(torch.mean(sy, axis=0)*1/2)\n",
    "\n",
    "\n",
    "def flip_neighbor_spins(samples, i):\n",
    "    s = samples.clone().detach()\n",
    "    N = s.size()[1]\n",
    "    s[:,i,:][samples[:,i,:] == 0]   = 1\n",
    "    s[:,i,:][samples[:,i,:] == 1]   = 0\n",
    "    s[:,(i+1)%N,:][samples[:,(i+1)%N,:] == 0] = 1\n",
    "    s[:,(i+1)%N,:][samples[:,(i+1)%N,:] == 1] = 0\n",
    "    return s\n",
    "\n",
    "def flip_spin(samples, i):\n",
    "    s = samples.clone().detach()\n",
    "    s[:,i,:][samples[:,i,:] == 0] = 1\n",
    "    s[:,i,:][samples[:,i,:] == 1] = 0\n",
    "    return s\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02f579eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/3000............. Loss: -1.22584364, Eloc: 0.76037025+0.01805130j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 20/3000............. Loss: 1.07746946, Eloc: -0.64664215-0.04899915j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 30/3000............. Loss: 0.50098293, Eloc: -1.00375688-0.08152942j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 40/3000............. Loss: -0.12644101, Eloc: -1.46408272-0.04905707j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 50/3000............. Loss: -0.74376801, Eloc: -1.68683684+0.04575497j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 60/3000............. Loss: -0.33300230, Eloc: -1.84665120-0.03551534j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 70/3000............. Loss: 0.48760876, Eloc: -1.92985451-0.01082754j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 80/3000............. Loss: 0.34291904, Eloc: -2.01398444-0.01791973j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 90/3000............. Loss: 0.11237989, Eloc: -2.07874727-0.00081262j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 100/3000............. Loss: -0.86212738, Eloc: -2.04057550+0.00340934j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 110/3000............. Loss: 0.74460445, Eloc: -2.28522444-0.01241680j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 120/3000............. Loss: -0.95341549, Eloc: -2.27642965+0.01458536j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 130/3000............. Loss: -0.36095510, Eloc: -2.48491168-0.00275895j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 140/3000............. Loss: 0.00758807, Eloc: -2.69607759-0.02114904j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 150/3000............. Loss: 0.40468943, Eloc: -2.80562758+0.03426909j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 160/3000............. Loss: -0.33412170, Eloc: -2.88412976-0.01241342j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 170/3000............. Loss: -0.30875897, Eloc: -2.86269522-0.00018194j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 180/3000............. Loss: 0.34593338, Eloc: -3.06267405-0.02698735j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 190/3000............. Loss: 0.14842805, Eloc: -3.00831199-0.04182532j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 200/3000............. Loss: 0.63465885, Eloc: -3.11962152-0.02318057j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 210/3000............. Loss: 0.14711536, Eloc: -3.16164422-0.00741726j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 220/3000............. Loss: 0.24206693, Eloc: -3.17159081+0.00383792j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 230/3000............. Loss: -0.16910387, Eloc: -3.14779639-0.02047965j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 240/3000............. Loss: -0.16688141, Eloc: -3.21676350+0.01746024j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 250/3000............. Loss: -0.04171165, Eloc: -3.20720935+0.00529777j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 260/3000............. Loss: -0.27165142, Eloc: -3.19670081-0.01404677j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 270/3000............. Loss: -0.12899911, Eloc: -3.25311804-0.01738234j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 280/3000............. Loss: 0.32456650, Eloc: -3.20638800-0.01237723j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 290/3000............. Loss: 0.06088425, Eloc: -3.26277733+0.00482982j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 300/3000............. Loss: -0.21432260, Eloc: -3.27891922+0.00243677j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 310/3000............. Loss: 0.25454674, Eloc: -3.27835894-0.01278436j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 320/3000............. Loss: -0.30227422, Eloc: -3.27436972+0.01410499j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 330/3000............. Loss: 0.22574765, Eloc: -3.26407623-0.03379996j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 340/3000............. Loss: 0.08349088, Eloc: -3.30262327+0.00445515j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 350/3000............. Loss: 0.21725291, Eloc: -3.30401278-0.01167342j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 360/3000............. Loss: -0.05919617, Eloc: -3.32188725+0.01099839j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 370/3000............. Loss: 0.22470484, Eloc: -3.31901073-0.01320223j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 380/3000............. Loss: 0.13923401, Eloc: -3.29899073+0.00093036j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 390/3000............. Loss: 0.07636341, Eloc: -3.32351136+0.00253349j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 400/3000............. Loss: -0.17950488, Eloc: -3.31934333-0.02199179j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 410/3000............. Loss: 0.05523306, Eloc: -3.34433889-0.00336923j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 420/3000............. Loss: -0.33652782, Eloc: -3.28895640-0.02205692j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 430/3000............. Loss: -0.01345367, Eloc: -3.32788277-0.00664545j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 440/3000............. Loss: -0.05334382, Eloc: -3.33118391+0.01199344j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 450/3000............. Loss: -0.09978622, Eloc: -3.32976460-0.00176331j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 460/3000............. Loss: 0.04979071, Eloc: -3.33211923+0.00621120j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 470/3000............. Loss: 0.16475624, Eloc: -3.33252621-0.01132703j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 480/3000............. Loss: 0.00097553, Eloc: -3.33042765-0.00063305j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 490/3000............. Loss: 0.19776663, Eloc: -3.34997439-0.00248569j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 500/3000............. Loss: -0.08209417, Eloc: -3.31956863-0.00463694j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 510/3000............. Loss: 0.13736842, Eloc: -3.34115124-0.00322408j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 520/3000............. Loss: 0.20340763, Eloc: -3.33370900+0.00368612j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 530/3000............. Loss: 0.15966801, Eloc: -3.33894658-0.00595487j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 540/3000............. Loss: 0.00130427, Eloc: -3.33654380-0.00566198j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 550/3000............. Loss: -0.04820452, Eloc: -3.32721877+0.00602535j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 560/3000............. Loss: 0.23355255, Eloc: -3.34687901-0.00149502j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 570/3000............. Loss: -0.29990815, Eloc: -3.33168101+0.00782931j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 580/3000............. Loss: 0.15365158, Eloc: -3.34548521+0.00072103j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 590/3000............. Loss: -0.12739538, Eloc: -3.34019780-0.00237808j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 600/3000............. Loss: 0.18170855, Eloc: -3.36010599-0.00505647j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 610/3000............. Loss: -0.16038676, Eloc: -3.35440660+0.00414515j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 620/3000............. Loss: -0.24663478, Eloc: -3.35630274-0.00269923j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 630/3000............. Loss: -0.00524845, Eloc: -3.34544754+0.00732863j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 640/3000............. Loss: -0.05050718, Eloc: -3.34245801-0.00066728j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 650/3000............. Loss: -0.69108004, Eloc: -3.31504345+0.00517604j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 660/3000............. Loss: 0.20320648, Eloc: -3.35690475+0.00231280j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 670/3000............. Loss: -0.16375031, Eloc: -3.33573246+0.00494912j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 680/3000............. Loss: 0.13967416, Eloc: -3.34184504+0.00541801j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 690/3000............. Loss: 0.23578730, Eloc: -3.36086416-0.00263130j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 700/3000............. Loss: 0.02556629, Eloc: -3.33546400-0.00101936j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 710/3000............. Loss: -0.09002290, Eloc: -3.34695172+0.00845076j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 720/3000............. Loss: 0.10539110, Eloc: -3.35723424+0.00325731j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 730/3000............. Loss: 0.19799699, Eloc: -3.36175990-0.00160827j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 740/3000............. Loss: -0.11093884, Eloc: -3.33080530+0.00139377j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 750/3000............. Loss: 0.27234864, Eloc: -3.36130404-0.00079217j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 760/3000............. Loss: -0.05964548, Eloc: -3.35179448+0.00151986j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 770/3000............. Loss: -0.05617069, Eloc: -3.31854296+0.00214619j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 780/3000............. Loss: 0.27263375, Eloc: -3.35360813+0.00001250j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 790/3000............. Loss: -0.26278077, Eloc: -3.32105899+0.00621956j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 800/3000............. Loss: 0.16000451, Eloc: -3.36148310-0.00078699j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 810/3000............. Loss: 0.02500549, Eloc: -3.34077764+0.00725039j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 820/3000............. Loss: 0.13510057, Eloc: -3.33792830-0.00220669j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 830/3000............. Loss: 0.15983151, Eloc: -3.35780501-0.00151000j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 840/3000............. Loss: -0.26803469, Eloc: -3.35937929+0.00370146j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 850/3000............. Loss: 0.41319039, Eloc: -3.36397409-0.00532357j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 860/3000............. Loss: -0.31280530, Eloc: -3.36429429+0.00117060j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 870/3000............. Loss: 0.20508864, Eloc: -3.34594154-0.00836274j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 880/3000............. Loss: -0.47073832, Eloc: -3.35535645+0.01487922j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 890/3000............. Loss: 0.24193978, Eloc: -3.34955215+0.00291399j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 900/3000............. Loss: -0.17209884, Eloc: -3.35528517-0.00208968j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 910/3000............. Loss: -0.34948906, Eloc: -3.36243796-0.00210776j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 920/3000............. Loss: -0.56640995, Eloc: -3.35215616-0.00059919j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 930/3000............. Loss: 0.24779009, Eloc: -3.34934163-0.00076923j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 940/3000............. Loss: 0.56253988, Eloc: -3.36097598-0.01817773j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 950/3000............. Loss: 0.13124615, Eloc: -3.35705757-0.00340860j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 960/3000............. Loss: -0.34234011, Eloc: -3.36117148-0.00064136j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 970/3000............. Loss: -0.19466778, Eloc: -3.36693025+0.01155675j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 980/3000............. Loss: 0.41817712, Eloc: -3.35651731-0.01012783j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 990/3000............. Loss: 0.10597419, Eloc: -3.36008883-0.00249630j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1000/3000............. Loss: -0.02377913, Eloc: -3.36293936+0.00121052j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1010/3000............. Loss: -0.03786329, Eloc: -3.36016560+0.01082853j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1020/3000............. Loss: -0.11394026, Eloc: -3.35716653+0.01645653j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1030/3000............. Loss: 0.14044931, Eloc: -3.36651754-0.00716893j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1040/3000............. Loss: -0.02841234, Eloc: -3.35686469+0.00169382j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1050/3000............. Loss: 0.18587621, Eloc: -3.37150216-0.00262446j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1060/3000............. Loss: -0.08243393, Eloc: -3.36519766+0.00218203j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1070/3000............. Loss: -0.17064015, Eloc: -3.36297226+0.01167760j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1080/3000............. Loss: -0.09551958, Eloc: -3.37011623+0.00671803j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1090/3000............. Loss: 0.10122289, Eloc: -3.35781527+0.00170699j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1100/3000............. Loss: -0.10184903, Eloc: -3.37482333-0.00168838j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1110/3000............. Loss: -0.01132068, Eloc: -3.35842776+0.01175931j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1120/3000............. Loss: 0.09526459, Eloc: -3.36698866-0.00079796j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1130/3000............. Loss: 0.03260074, Eloc: -3.35921621-0.00321186j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1140/3000............. Loss: 0.22395380, Eloc: -3.37367868-0.00404910j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1150/3000............. Loss: -0.16376972, Eloc: -3.36746240-0.00178794j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1160/3000............. Loss: 0.19900769, Eloc: -3.37064743-0.00634987j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1170/3000............. Loss: 0.03304263, Eloc: -3.37117386+0.00119928j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1180/3000............. Loss: -0.14973925, Eloc: -3.36122441+0.00576373j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1190/3000............. Loss: -0.13785092, Eloc: -3.37239289-0.00010364j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1200/3000............. Loss: 0.06821206, Eloc: -3.35055327+0.00865643j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1210/3000............. Loss: -0.18010732, Eloc: -3.37575579+0.00172151j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1220/3000............. Loss: 0.22002929, Eloc: -3.36784267-0.00572091j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1230/3000............. Loss: -0.19869699, Eloc: -3.37443900-0.00545299j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1240/3000............. Loss: 0.17399560, Eloc: -3.37083650-0.00855501j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1250/3000............. Loss: -0.04367356, Eloc: -3.36552668+0.00035903j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1260/3000............. Loss: 0.00961608, Eloc: -3.37111974-0.00844882j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1270/3000............. Loss: -0.04320549, Eloc: -3.37132239-0.00548580j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1280/3000............. Loss: 0.20379680, Eloc: -3.37054086+0.00482853j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1290/3000............. Loss: 0.12521916, Eloc: -3.36668658-0.00374488j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1300/3000............. Loss: -0.09544933, Eloc: -3.36779690+0.00048518j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1310/3000............. Loss: 0.01994364, Eloc: -3.37246966-0.00395561j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1320/3000............. Loss: -0.00094061, Eloc: -3.37465525+0.00298562j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1330/3000............. Loss: -0.11894551, Eloc: -3.37757277+0.00576748j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1340/3000............. Loss: -0.02212464, Eloc: -3.37052774+0.00672661j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1350/3000............. Loss: -0.05210697, Eloc: -3.37388921-0.00496147j, Sx: 0.0000, Sy: 0.0000, Sz: -0.0000\n",
      "Epoch: 1360/3000............. Loss: -0.03935872, Eloc: -3.37497044+0.00062357j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1370/3000............. Loss: 0.38575528, Eloc: -3.37167192-0.00557592j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1380/3000............. Loss: 0.00840804, Eloc: -3.37604451-0.00098246j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1390/3000............. Loss: -0.00700954, Eloc: -3.37522793+0.00221334j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1400/3000............. Loss: -0.12223184, Eloc: -3.37212253+0.00545601j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1410/3000............. Loss: -0.07419326, Eloc: -3.36988354+0.00527110j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1420/3000............. Loss: -0.19000646, Eloc: -3.37277508+0.00849370j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1430/3000............. Loss: -0.11672668, Eloc: -3.37012148-0.00384983j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1440/3000............. Loss: -0.05884027, Eloc: -3.37209773+0.00172562j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1450/3000............. Loss: 0.13312392, Eloc: -3.37333322+0.00101196j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1460/3000............. Loss: -0.12958286, Eloc: -3.36630225-0.00136119j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1470/3000............. Loss: 0.08589632, Eloc: -3.37504721+0.00172611j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1480/3000............. Loss: 0.00914128, Eloc: -3.36880708-0.00230453j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n",
      "Epoch: 1490/3000............. Loss: 0.17061553, Eloc: -3.37500048-0.00055916j, Sx: 0.0000, Sy: 0.0000, Sz: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_3388/3419788084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_3388/3270727616.py\u001b[0m in \u001b[0;36mget_sy\u001b[0;34m(samples, log_probs, phases)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflip_spin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mlog_probs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphases1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_3388/3754483022.py\u001b[0m in \u001b[0;36mlog_probabilities\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mampl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mampl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msz_tot\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mampl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_sz_total\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mampl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mampl_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mampl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kj/w16m6_q14ln6kd7tkfk1s0780000gp/T/ipykernel_3388/3754483022.py\u001b[0m in \u001b[0;36menforce_sz_total\u001b[0;34m(self, samples, amplitudes)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menforce_sz_total\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamplitudes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0ms_dn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0ms_dn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 500\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    samples, log_probs, phases = model.sample(n_samples)\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    \n",
    "    Eloc, cost = cost_fct(samples, model, Jp, Jz, log_probs, phases, bounds)\n",
    "    cost.backward(retain_graph=True) # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    optimizer.zero_grad()\n",
    "    sx = get_sx(samples, log_probs, phases)\n",
    "    sy = get_sy(samples, log_probs, phases)\n",
    "    sz = get_sz(samples)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.8f}\".format(cost)+\", Eloc: {:.8f}\".format(Eloc)+\", Sx: {:.4f}\".format(sx)+\", Sy: {:.4f}\".format(sy)+\", Sz: {:.4f}\".format(sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5399d598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 1, 0, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0, 1, 0, 1],\n",
      "        [1, 1, 0, 0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 1, 0, 1, 0]])\n",
      "tensor([ 0.8028-0.5963j,  0.7943-0.6076j,  0.7928-0.6095j,  0.7928-0.6095j,\n",
      "         0.7943-0.6076j,  0.7928-0.6095j,  0.7943-0.6076j, -0.7866+0.6175j,\n",
      "         0.7878-0.6160j,  0.7878-0.6160j, -0.8140+0.5809j, -0.7980+0.6026j,\n",
      "         0.7878-0.6160j,  0.7878-0.6160j,  0.7928-0.6095j, -0.8098+0.5867j,\n",
      "        -0.8213+0.5705j,  0.7943-0.6076j,  0.7943-0.6076j, -0.7982+0.6024j,\n",
      "        -0.7964+0.6047j,  0.7878-0.6160j, -0.8140+0.5809j,  0.7943-0.6076j,\n",
      "         0.8156-0.5787j,  0.7878-0.6160j,  0.7943-0.6076j, -0.8140+0.5809j,\n",
      "        -0.8140+0.5809j,  0.7943-0.6076j,  0.7878-0.6160j,  0.7878-0.6160j,\n",
      "         0.7928-0.6095j, -0.8140+0.5809j, -0.7924+0.6100j, -0.7924+0.6100j,\n",
      "        -0.7945+0.6073j,  0.7943-0.6076j, -0.7976+0.6033j, -0.8037+0.5950j,\n",
      "         0.7878-0.6160j,  0.7463-0.6656j,  0.7878-0.6160j, -0.7539+0.6570j,\n",
      "         0.7982-0.6024j, -0.8140+0.5809j,  0.7928-0.6095j, -0.8243+0.5661j,\n",
      "         0.7878-0.6160j, -0.8037+0.5950j], dtype=torch.complex128,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([0.0944, 0.4997, 0.3101, 0.3101, 0.4997, 0.3101, 0.4997, 0.0803, 0.4867,\n",
      "        0.4867, 0.3357, 0.2416, 0.4867, 0.4867, 0.3101, 0.0591, 0.2476, 0.4997,\n",
      "        0.4997, 0.1515, 0.2423, 0.4867, 0.3357, 0.4997, 0.3193, 0.4867, 0.4997,\n",
      "        0.3357, 0.3357, 0.4997, 0.4867, 0.4867, 0.3101, 0.3357, 0.3365, 0.3365,\n",
      "        0.3343, 0.4997, 0.1452, 0.3353, 0.4867, 0.0468, 0.4867, 0.0447, 0.2335,\n",
      "        0.3357, 0.3101, 0.1562, 0.4867, 0.3353], dtype=torch.float64,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]])\n",
      "0.499737994099005\n"
     ]
    }
   ],
   "source": [
    "samples, log_probs, phases = model.sample(1000)\n",
    "print(torch.reshape(samples, (1000,systemsize))[:10])\n",
    "print(torch.exp(1j*phases)[:50])\n",
    "print(torch.exp(0.5*log_probs)[:50])\n",
    "print(samples[np.argmax(torch.exp(0.5*log_probs).detach().numpy())])\n",
    "print(max(torch.exp(0.5*log_probs).detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181edff",
   "metadata": {},
   "source": [
    "### 3. Evaluate Observables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53af738",
   "metadata": {},
   "source": [
    "This is what we can test:\n",
    "- For the Heisenberg model:The average magnetization in all directions should vanish.\n",
    "- The correlator in all directions should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ae440a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 7])\n",
      "tensor([-0.2200, -0.0845, -0.1905, -0.1085, -0.1790, -0.0890, -0.2095])\n",
      "tensor([-0.2247, -0.0927, -0.1924, -0.1163, -0.1903, -0.1111, -0.2182],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([-0.2247, -0.0927, -0.1924, -0.1163, -0.1903, -0.1111, -0.2182],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate the nearest neighbor spin correlators\n",
    "samples, log_probs, phases = model.sample(1000)\n",
    "szsz = get_szsz(samples, log_probs, bounds)\n",
    "print(szsz)\n",
    "sxsx = get_sxsx(samples, log_probs, phases, bounds)\n",
    "print(sxsx)\n",
    "sysy = get_sysy(samples, log_probs, phases, bounds)\n",
    "print(sysy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a0cf907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.7347e-18, dtype=torch.float64)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate sx, sy and sz\n",
    "sz = get_sz(samples)\n",
    "print(sz)\n",
    "sx = get_sx(samples, log_probs, phases)\n",
    "print(sx)\n",
    "sy = get_sy(samples, log_probs, phases)\n",
    "print(sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a3d95f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 7])\n"
     ]
    }
   ],
   "source": [
    "def save(model, boundaries, folder):\n",
    "    torch.save(model.state_dict(), folder+\"model_params.pt\")\n",
    "    # calculate the nearest neighbor spin correlators\n",
    "    samples, log_probs, phases = model.sample(1000)\n",
    "    szsz = get_szsz(samples, log_probs, boundaries).detach().numpy()\n",
    "    np.save(folder+\"szsz.npy\", szsz)\n",
    "    sxsx = get_sxsx(samples, log_probs, phases, boundaries).detach().numpy()\n",
    "    np.save(folder+\"sxsx.npy\", sxsx)\n",
    "    sysy = get_sysy(samples, log_probs, phases, boundaries).detach().numpy()\n",
    "    np.save(folder+\"sysy.npy\", sysy)\n",
    "\n",
    "save(model, bounds, \"with_total_sz=0/Delta=0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model can then be load again by using\n",
    "#model = Model(input_size=2, system_size=systemsize, hidden_dim=hiddendim, n_layers=1)\n",
    "#model.load_state_dict(torch.load(\"model_params.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba26767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
